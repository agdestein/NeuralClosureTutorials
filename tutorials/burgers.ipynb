{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural closure models for the viscous Burgers equation\n",
    "\n",
    "In this tutorial, we will train neural closure models for the viscous Burgers\n",
    "equation in the [Julia](https://julialang.org/) programming language. We here\n",
    "use Julia for ease of use, efficiency, and writing differentiable code to\n",
    "power scientific machine learning. This file is available as\n",
    "\n",
    "- a commented [Julia script](https://github.com/agdestein/NeuralClosure/blob/main/tutorials/burgers.jl),\n",
    "- a [markdown file](https://github.com/agdestein/NeuralClosure/blob/main/tutorials/burgers.md),\n",
    "- a Jupyter [notebook](https://github.com/agdestein/NeuralClosure/blob/main/tutorials/burgers.ipynb)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running this notebook on Google Colab\n",
    "\n",
    "_This section is only needed when running on Google colab._\n",
    "_If you run this notebook on your local machine, skip this section._\n",
    "\n",
    "To use Julia on Google colab, we will install Julia using the official version\n",
    "manager Juliup. From the default Python runtime, we can access the shell by\n",
    "starting a line with `!`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "!curl -fsSL https://install.julialang.org | sh -s -- --yes"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can check that Julia is successfully installed on the Colab instance."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "!~/.juliaup/bin/julia -e 'println(\"Hello\")'"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now proceed to install the necessary Julia packages, including `IJulia` which\n",
    "will add the Julia notebook kernel."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "%%shell\n",
    "~/.juliaup/bin/julia -e '''\n",
    "    using Pkg\n",
    "    Pkg.add([\n",
    "        \"ComponentArrays\",\n",
    "        \"FFTW\",\n",
    "        \"IJulia\",\n",
    "        \"LinearAlgebra\",\n",
    "        \"Lux\",\n",
    "        \"NNlib\",\n",
    "        \"Optimisers\",\n",
    "        \"Plots\",\n",
    "        \"Printf\",\n",
    "        \"Random\",\n",
    "        \"SparseArrays\",\n",
    "        \"Zygote\",\n",
    "    ])\n",
    "'''"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once this cell has finished running (this may take a few minutes,\n",
    "depending on what resources Colab decides to give you), do the following:\n",
    "\n",
    "1. Reload the browser page (`CTRL`/`CMD` + `R`)\n",
    "2. In the top right corner of Colab, then select the Julia kernel.\n",
    "\n",
    "   ![](https://github.com/agdestein/NeuralClosure/blob/main/assets/select.png?raw=true)\n",
    "\n",
    "   ![](https://github.com/agdestein/NeuralClosure/blob/main/assets/runtime.png?raw=true)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the simulations\n",
    "\n",
    "Julia comes with many built in features, including array functionality.\n",
    "Additional functionality is provided in various packages, some of which are\n",
    "available in the Standard Library (LinearAlgebra, Printf, Random,\n",
    "SparseArrays). Others are available in the General Registry, and can be added\n",
    "using the built in package manager Pkg, e.g. `using Pkg; Pkg.add(\"Plots\")`.\n",
    "If you ran the Colab setup section, the packages should already be added.\n",
    "\n",
    "If you cloned the NeuralClosure repository and run this in VSCode,\n",
    "there should be a file called `Project.toml`, specifying dependencies.\n",
    "This file specifies an environment, which can be activated.\n",
    "**If you are running this locally (not on Colab)**:\n",
    "You can then install the dependencies by uncommenting and running the\n",
    "following cell:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# using Pkg\n",
    "# Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alernatively, you can add them manually:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# using Pkg\n",
    "# Pkg.add([\n",
    "#     \"ComponentArrays\",\n",
    "#     \"FFTW\",\n",
    "#     \"LinearAlgebra\",\n",
    "#     \"Lux\",\n",
    "#     \"NNlib\",\n",
    "#     \"Optimisers\",\n",
    "#     \"Plots\",\n",
    "#     \"Printf\",\n",
    "#     \"Random\",\n",
    "#     \"SparseArrays\",\n",
    "#     \"Zygote\",\n",
    "# ])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ComponentArrays\n",
    "using FFTW\n",
    "using LinearAlgebra\n",
    "using Lux\n",
    "using NNlib\n",
    "using Optimisers\n",
    "using Plots\n",
    "using Printf\n",
    "using Random\n",
    "using SparseArrays\n",
    "using Zygote"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we have loaded the reverse mode AD framework\n",
    "[Zygote](https://github.com/FluxML/Zygote.jl). This package provides the\n",
    "function `gradient`, which is able to differentiate functions that we write,\n",
    "including our numerical solvers (to be defined in the next section).\n",
    "Gradients allow us to perform gradient descent, and thus \"train\" our neural\n",
    "networks.\n",
    "\n",
    "The deep learning framework [Lux](https://lux.csail.mit.edu/) likes to toss\n",
    "random number generators (RNGs) around, for reproducible science. We\n",
    "therefore need to initialize an RNG. The seed makes sure the same sequence of\n",
    "pseudo-random numbers are generated at each time the session is restarted."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(123)\n",
    "rng = Random.default_rng()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Deep learning functions usually use single precision floating point numbers\n",
    "by default, as this is preferred on GPUs. Julia itself, on the other hand, is\n",
    "slightly opinionated towards double precision floating point numbers, e.g.\n",
    "`3.14`, `1 / 6` and `2π` are all of type `Float64` (their single precision\n",
    "counterparts would be the slightly more verbose `3.14f0`, `1f0 / 6` (or\n",
    "`Float32(1 / 6)`) and `2f0π`). For simplicity, we will only use `Float64`.\n",
    "The only place we will encounter `Float32` in this file is in the default\n",
    "neural network weight initializers, so here is an alternative weight\n",
    "initializer using double precision."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "glorot_uniform_64(rng::AbstractRNG, dims...) = glorot_uniform(rng, Float64, dims...)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The viscous Burgers equation\n",
    "\n",
    "Consider the periodic domain $\\Omega = [0, 1]$. The viscous Burgers equation\n",
    "is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} = - \\frac{1}{2} \\frac{\\partial }{\\partial x}\n",
    "\\left( u^2 \\right) + \\mu \\frac{\\partial^2 u}{\\partial x^2},\n",
    "$$\n",
    "\n",
    "where $\\mu > 0$ is the viscosity. The Burgers equation models the velocity\n",
    "profile of a compressible fluid, and has the particularity of creating\n",
    "shocks, which are dampened by the viscosity.\n",
    "\n",
    "### Discretization\n",
    "\n",
    "For simplicity, we will use a uniform discretization\n",
    "$x = \\left( \\frac{n}{N} \\right)_{n = 1}^N$,\n",
    "with the additional \"ghost\" points $x_0 = 0$ overlapping with\n",
    "$x_N$ and $x_{N + 1}$ overlapping with $x_1$.\n",
    "The grid spacing is $\\Delta x = \\frac{1}{N}$.\n",
    "Using a central finite difference, we get the discrete equations\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} u_n}{\\mathrm{d} t} = - \\frac{1}{2} \\frac{u_{n + 1}^2 - u_{n -\n",
    "1}^2}{2 \\Delta x} + \\mu \\frac{u_{n + 1} - 2 u_n + u_{n - 1}}{\\Delta x^2}\n",
    "$$\n",
    "\n",
    "for all $n \\in \\{1, \\dots, N\\}$, with the convention $u_0 = u_N$ and $u_{N +\n",
    "1} = u_1$ (periodic extension). The degrees of freedom are stored in the\n",
    "vector $u = (u_n)_{n = 1}^N$. In vector notation, we will write this as\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} u}{\\mathrm{d} t} = f_\\text{central}(u).\n",
    "$$\n",
    "\n",
    "Note that $f_\\text{central}$ is not  a good discretization for\n",
    "dealing with shocks. Jameson [^1] proposes the following scheme instead:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\mathrm{d} u_n}{\\mathrm{d} t} & = - \\frac{\\phi_{n + 1 / 2} - \\phi_{n - 1 / 2}}{\\Delta x}, \\\\\n",
    "\\phi_{n + 1 / 2} & = \\frac{u_{n + 1}^2 + u_{n + 1} u_n + u_n^2}{6} - \\mu_{n + 1 / 2} \\frac{u_{n + 1} - u_n}{\\Delta x}, \\\\\n",
    "\\mu_{n + 1 / 2} & = \\mu + \\Delta x \\left( \\frac{| u_{n + 1} + u_n |}{4} - \\frac{u_{n + 1} - u_n}{12} \\right),\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $ϕ_{n + 1 / 2}$ is the numerical flux from $u_n$ to $u_{n + 1}$\n",
    "and $\\mu_{n + 1 / 2}$ includes the original viscosity and a numerical viscosity.\n",
    "This prevents oscillations near shocks. In vector notation,\n",
    "we will denote this right hand side by $f_\\text{shock}$.\n",
    "\n",
    "Solving the equation $\\frac{\\mathrm{d} u}{\\mathrm{d} t} = f(u)$ for sufficiently\n",
    "small $\\Delta x$ (sufficiently large $N$) will be referred to as\n",
    "_direct numerical simulation_ (DNS). DNS can be expensive, in particular for\n",
    "more complicated equations such as the Navier-Stokes equations in 2D/3D.\n",
    "\n",
    "We start by defining the right hand side function $f$ for a vector $u$, making\n",
    "sure to account for the periodic boundaries. The macro `@.` makes sure that all\n",
    "following operations are performed element-wise. Note that `circshift` here\n",
    "acts along the first dimension, so `f` can be applied to multiple snapshots\n",
    "at once (stored as columns in the matrix `u`). Note the semicolon `;` in the\n",
    "function signature: It is used to separate positional arguments (here `u`)\n",
    "from keyword arguments (here `μ`)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function f_central(u; μ)\n",
    "    Δx = 1 / size(u, 1)\n",
    "    u₊ = circshift(u, -1)\n",
    "    u₋ = circshift(u, 1)\n",
    "    @. -(u₊^2 - u₋^2) / 4Δx + μ * (u₊ - 2u + u₋) / Δx^2\n",
    "end\n",
    "\n",
    "function f_shock(u; μ)\n",
    "    Δx = 1 / size(u, 1)\n",
    "    u₊ = circshift(u, -1)\n",
    "    μ₊ = @. μ + Δx * abs(u + u₊) / 4 - Δx * (u₊ - u) / 12\n",
    "    ϕ₊ = @. (u^2 + u * u₊ + u₊^2) / 6 - μ₊ * (u₊ - u) / Δx\n",
    "    ϕ₋ = circshift(ϕ₊, 1)\n",
    "    @. -(ϕ₊ - ϕ₋) / Δx\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Time discretization\n",
    "\n",
    "For time stepping, we do a simple explicit Runge-Kutta scheme (RK).\n",
    "\n",
    "From a current state $u^0 = u(t)$, we divide the outer time step\n",
    "$\\Delta t$ into $i \\in \\{1, \\dots, s\\}$ sub-steps as follows:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "f^i & = f(u^{i - 1}) \\\\\n",
    "u^i & = u^0 + \\Delta t \\sum_{j = 1}^{i} A_{i j} f^j,\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $A \\in \\mathbb{R}^{s \\times s}$ are the coefficients of the RK method.\n",
    "The solution at the next outer time step $t + \\Delta t$ is then $u^s = u(t +\n",
    "\\Delta t) + \\mathcal{O}(\\Delta t^{r + 1})$ if we start exactly from $u(t)$,\n",
    "where $r$ is the order of the RK method. If we chain multiple steps from the\n",
    "initial conditions $u(0)$ to a final state $u(t)$, the total error is of\n",
    "order $\\mathcal{O}(\\Delta t^r)$.\n",
    "\n",
    "A fourth order method is given by the following coefficients ($s = 4$, $r =\n",
    "4$):\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "    \\frac{1}{2} & 0           & 0           & 0 \\\\\n",
    "    0           & \\frac{1}{2} & 0           & 0 \\\\\n",
    "    0           & 0           & 1           & 0 \\\\\n",
    "    \\frac{1}{6} & \\frac{2}{6} & \\frac{2}{6} & \\frac{1}{6}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "_Note: The Runge-Kutta coefficients are usually presented in a shifted\n",
    "version of our $A$, and also includes two vectors $c$ and $b$. Since our\n",
    "system is autonomous and the RK scheme is explicit, we can write it in this\n",
    "simple form._\n",
    "\n",
    "The following function performs one RK4 time step. Note that we never modify\n",
    "any vectors, only create new ones. The AD-framework Zygote prefers it this\n",
    "way. The splatting syntax `params...` lets us pass a variable number of\n",
    "keyword arguments to the right hand side function `f` (for the above `f`\n",
    "there is only one: `μ`). Similarly, `k...` splats the tuple `k`, but but now\n",
    "like a positional argument instead of keyword arguments (without names)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function step_rk4(f, u₀, dt; params...)\n",
    "    A = [\n",
    "        1/2 0 0 0\n",
    "        0 1/2 0 0\n",
    "        0 0 1 0\n",
    "        1/6 2/6 2/6 1/6\n",
    "    ]\n",
    "    u = u₀\n",
    "    k = ()\n",
    "    for i = 1:size(A, 1)\n",
    "        ki = f(u; params...)\n",
    "        k = (k..., ki)\n",
    "        u = u₀\n",
    "        for j = 1:i\n",
    "            u = u + dt * A[i, j] * k[j]\n",
    "        end\n",
    "    end\n",
    "    u\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Solving the ODE is done by chaining individual time steps. We here add the\n",
    "option to call a callback function after each time step. Note that the path\n",
    "to the final output `u` is obtained by passing the inputs `u₀` and\n",
    "parameters `params` through a finite amount of computational steps, each of\n",
    "which should have a chain rule defined and recognized in the Zygote AD\n",
    "framework. The ODE solution `u` should be differentiable (with respect to\n",
    "`u₀` or `params`), as long as `f` is."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function solve_ode(f, u₀; dt, nt, callback = (u, t, i) -> nothing, ncallback = 1, params...)\n",
    "    t = 0.0\n",
    "    u = u₀\n",
    "    callback(u, t, 0)\n",
    "    for i = 1:nt\n",
    "        t += dt\n",
    "        u = step_rk4(f, u, dt; params...)\n",
    "        i % ncallback == 0 && callback(u, t, i)\n",
    "    end\n",
    "    u\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initial conditions\n",
    "\n",
    "For the initial conditions, we use the following random Fourier series:\n",
    "\n",
    "$$\n",
    "u_0(x) = \\mathfrak{R} \\sum_{k = -k_\\text{max}}^{k_\\text{max}} c_k\n",
    "\\mathrm{e}^{2 \\pi \\mathrm{i} k x},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\mathfrak{R}$ denotes the real part\n",
    "- $c_k = a_k d_k \\mathrm{e}^{- 2 \\pi \\mathrm{i} b_k}$ are random\n",
    "  Fourier series coefficients\n",
    "- $a_k \\sim \\mathcal{N}(0, 1)$ is a normally distributed random amplitude\n",
    "- $d_k = (1 + | k |)^{- 6 / 5}$ is a deterministic spectral decay profile,\n",
    "  so that the large scale features dominate the initial flow\n",
    "- $b_k \\sim \\mathcal{U}(0, 1)$ is a uniform random phase shift between 0 and 1\n",
    "- $\\mathrm{e}^{2 \\pi \\mathrm{i} k x}$ is a sinusoidal Fourier series basis\n",
    "  function evaluated at the point $x \\in \\Omega$\n",
    "\n",
    "Note in particular that the constant coefficient $c_0$ ($k = 0$) is almost\n",
    "certainly non-zero, and with complex amplitude $| c_0 | = | a_0 |$.\n",
    "\n",
    "Since the same Fourier basis can be reused multiple times, we write a\n",
    "function that creates multiple initial condition samples in one go. Each\n",
    "discrete $u_0$ vector is stored as a column in the resulting matrix."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function create_initial_conditions(\n",
    "    nx,\n",
    "    nsample;\n",
    "    kmax = 16,\n",
    "    decay = k -> (1 + abs(k))^(-6 / 5),\n",
    ")\n",
    "    # Fourier basis\n",
    "    basis = [exp(2π * im * k * x / nx) for x ∈ 1:nx, k ∈ -kmax:kmax]\n",
    "\n",
    "    # Fourier coefficients with random phase and amplitude\n",
    "    c = [randn() * exp(-2π * im * rand()) * decay(k) for k ∈ -kmax:kmax, _ ∈ 1:nsample]\n",
    "\n",
    "    # Random data samples (real-valued)\n",
    "    # Note the matrix product for summing over $k$\n",
    "    real.(basis * c)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example simulation\n",
    "\n",
    "Let's test our method in action. The animation\n",
    "syntax will create one animated GIF from a collection of frames.\n",
    "\n",
    "This is also the point where we have to provide some parameters, including\n",
    "\n",
    "- `nx`: Number of grid points\n",
    "- `μ`: Viscosity\n",
    "- `dt`: Time step increment\n",
    "- `nt`: Number of time steps (final time is `nt * dt`)\n",
    "- `ncallback`: Number of time steps between plot frames in the animation"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nx = 1024\n",
    "x = LinRange(0.0, 1.0, nx + 1)[2:end]\n",
    "\n",
    "# Initial conditions (one sample vector)\n",
    "u₀ = create_initial_conditions(nx, 1)\n",
    "# u₀ = sin.(2π .* x)\n",
    "# u₀ = sin.(4π .* x)\n",
    "# u₀ = sin.(6π .* x)\n",
    "\n",
    "# Time stepping\n",
    "anim = Animation()\n",
    "u = solve_ode(\n",
    "    f_shock,\n",
    "    # f_central,\n",
    "    u₀;\n",
    "    μ = 5.0e-4,\n",
    "    dt = 1.0e-4,\n",
    "    nt = 5000,\n",
    "    ncallback = 50,\n",
    "    callback = (u, t, i) -> frame(\n",
    "        anim,\n",
    "        plot(\n",
    "            x,\n",
    "            u;\n",
    "            xlabel = \"x\",\n",
    "            ylims = extrema(u₀),\n",
    "            # marker = :o,\n",
    "            title = @sprintf(\"Solution, t = %.3f\", t)\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "gif(anim)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is typical for the Burgers equation: The initial conditions merge to\n",
    "form a shock, which is eventually dampened due to the viscosity. If we let\n",
    "the simulation go on, diffusion will take over and we get a smooth solution\n",
    "and eventually $u$ becomes constant."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise: Understanding the Burgers equation\n",
    "\n",
    "The animation shows the velocity profile of a fluid.\n",
    "\n",
    "- In which direction is the fluid moving?\n",
    "- Rerun the above cell a couple of times to get different random initial\n",
    "  conditions. Does the velocity always move in the same direction?\n",
    "- Find a region where $u > 0$. In what direction is the curve moving?\n",
    "- Find a region where $u < 0$. In what direction is the curve moving?\n",
    "- Find a point where $u = 0$. Does this point move?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise: Understanding the discretization\n",
    "\n",
    "Choose one of the sine waves as initial conditions.\n",
    "Change the resolution to `nx = 128`.\n",
    "Run once with `f_shock` and once with `f_central`.\n",
    "You can try with and without the `marker = :o` keyword to see\n",
    "the discrete points better.\n",
    "\n",
    "Questions:\n",
    "\n",
    "- Is there something strange with `f_central`?\n",
    "- On which side(s) of the shock is there a problem?\n",
    "- Does this problem go away when you increase `nx`?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem set-up for large eddy simulation (LES) with neural networks\n",
    "\n",
    "We now assume that we are only interested in the large scale structures of the\n",
    "flow. To compute those, we would ideally like to use a coarser resolution\n",
    "($N_\\text{LES}$) than the one needed to resolve all the features of the flow\n",
    "($N_\\text{DNS}$).\n",
    "\n",
    "To define \"large scales\", we consider a discrete filter $\\Phi \\in\n",
    "\\mathbb{R}^{N_\\text{LES} \\times N_\\text{DNS}}$, averaging multiple DNS grid\n",
    "points into LES points. The filtered velocity field is defined by $\\bar{u} =\n",
    "\\Phi u$. It is governed by the equation\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} \\bar{u}}{\\mathrm{d} t} = f(\\bar{u}) + c(u, \\bar{u}),\n",
    "$$\n",
    "\n",
    "where $f$ is adapted to the grid of its input field $\\bar{u}$ and\n",
    "$c(u, \\bar{u}) = \\overline{f(u)} - f(\\bar{u})$ is the commutator error\n",
    "between the coarse grid and filtered fine grid right hand sides. Given\n",
    "$\\bar{u}$ only, this commutator error is **unknown**, and the equation\n",
    "needs a closure model.\n",
    "\n",
    "To close the equations, we approximate the unknown commutator error using a\n",
    "closure model $m$ with parameters $\\theta$:\n",
    "\n",
    "$$\n",
    "m(\\bar{u}, \\theta) \\approx c(u, \\bar{u}).\n",
    "$$\n",
    "\n",
    "We need to make two choices: $m$ and $\\theta$. While the model $m$ will\n",
    "be chosen based on our expertise of numerical methods and machine learning,\n",
    "the parameters $\\theta$ will be fitted using high fidelity simulation data.\n",
    "We can then solve the LES equation\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} \\bar{v}}{\\mathrm{d} t} = f(\\bar{v}) + m(\\bar{v}, θ),\n",
    "$$\n",
    "\n",
    "where the LES solution $\\bar{v}$ is an approximation to the filtered DNS\n",
    "solution $\\bar{u}$, starting from the exact initial conditions $\\bar{v}(0) =\n",
    "\\bar{u}(0)$.\n",
    "\n",
    "Since the 1D Burgers equation does not create \"turbulence\" in the same way\n",
    "as the 3D Navier-Stokes equations do, we choose to use a discretization\n",
    "that is \"bad\" on coarse grids (the LES grid), but still performs fine on fine\n",
    "grids (the DNS grid). This will give the neural network closure models some\n",
    "work to do."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "dns(u; μ) = f_central(u; μ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following right hand side function includes the correction term, and thus\n",
    "constitutes the LES right hand side."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "les(u; μ, m, θ) = dns(u; μ) + m(u, θ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data generation\n",
    "\n",
    "This generic function creates a data structure containing filtered DNS data,\n",
    "commutator errors and simulation parameters for a given filter $Φ$. We also\n",
    "provide some default values for the initial conditions."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function create_data(nsample, Φ; μ, dt, nt, kmax = 10, decay = k -> (1 + abs(k))^(-6 / 5))\n",
    "    # Resolution\n",
    "    nx_les, nx_dns = size(Φ)\n",
    "\n",
    "    # Grids\n",
    "    x_les = LinRange(0.0, 1.0, nx_les + 1)[2:end]\n",
    "    x_dns = LinRange(0.0, 1.0, nx_dns + 1)[2:end]\n",
    "\n",
    "    # Output data\n",
    "    data = (;\n",
    "        # Filtered snapshots and commutator errors (including at t = 0)\n",
    "        u = zeros(nx_les, nsample, nt + 1),\n",
    "        c = zeros(nx_les, nsample, nt + 1),\n",
    "\n",
    "        # Simulation-specific parameters\n",
    "        dt,\n",
    "        μ,\n",
    "    )\n",
    "\n",
    "    # DNS Initial conditions\n",
    "    u₀ = create_initial_conditions(nx_dns, nsample; kmax, decay)\n",
    "\n",
    "    # Save filtered solution and commutator error after each DNS time step\n",
    "    function callback(u, t, i)\n",
    "        ubar = Φ * u\n",
    "        data.u[:, :, i+1] = ubar\n",
    "        data.c[:, :, i+1] = Φ * dns(u; μ) - dns(ubar; μ)\n",
    "    end\n",
    "\n",
    "    # Do DNS time stepping (save after each step)\n",
    "    u = solve_ode(dns, u₀; μ, dt, nt, callback, ncallback = 1)\n",
    "\n",
    "    # Return data\n",
    "    data\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we set up an experiment. We need to decide on the following:\n",
    "\n",
    "- Problem parameter: $\\mu$\n",
    "- LES resolution\n",
    "- DNS resolution\n",
    "- Discrete filter\n",
    "- Number of initial conditions\n",
    "- Simulation time: Too short, and we won't have time to detect instabilities\n",
    "  created by our model; too long, and most of the data will be too smooth for\n",
    "  a closure model to be needed (due to viscosity)\n",
    "\n",
    "In addition, we will split our data into\n",
    "\n",
    "- Training data (for choosing $\\theta$)\n",
    "- Validation data (just for monitoring training, choose when to stop)\n",
    "- Testing data (for testing performance on unseen data)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Resolution\n",
    "nx_les = 64\n",
    "nx_dns = 1024\n",
    "\n",
    "# Grids\n",
    "x_les = LinRange(0.0, 1.0, nx_les + 1)[2:end]\n",
    "x_dns = LinRange(0.0, 1.0, nx_dns + 1)[2:end]\n",
    "\n",
    "# Grid sizes\n",
    "Δx_les = 1 / nx_les\n",
    "Δx_dns = 1 / nx_dns"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use a Gaussian filter kernel, truncated to zero outside of $3 / 2$\n",
    "filter widths."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Filter width\n",
    "ΔΦ = 5 * Δx_les\n",
    "\n",
    "# Filter kernel\n",
    "gaussian(Δ, x) = sqrt(6 / π) / Δ * exp(-6x^2 / Δ^2)\n",
    "top_hat(Δ, x) = (abs(x) ≤ Δ / 2) / Δ\n",
    "kernel = gaussian\n",
    "\n",
    "# Discrete filter matrix (with periodic extension and threshold for sparsity)\n",
    "Φ = sum(-1:1) do z\n",
    "    d = @. x_les - x_dns' - z\n",
    "    @. kernel(ΔΦ, d) * (abs(d) ≤ 3 / 2 * ΔΦ)\n",
    "end\n",
    "Φ = Φ ./ sum(Φ; dims = 2) ## Normalize weights\n",
    "Φ = sparse(Φ)\n",
    "dropzeros!(Φ)\n",
    "heatmap(Φ; yflip = true, xmirror = true, title = \"Filter matrix\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To illustrate the closure problem, we will run an LES simulation without a closure model.\n",
    "Consider the following setup:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "μ = 5.0e-4\n",
    "u = sin.(6π * x_dns)\n",
    "ubar = Φ * u\n",
    "vbar = ubar\n",
    "dt = 1.0e-4\n",
    "anim = Animation()\n",
    "for it = 0:5000\n",
    "    # Only needed if we are not running interactively\n",
    "    # (since we are not inside a function) e.g. on GitHub\n",
    "    # Note: These variables are still local to the cell\n",
    "    global u, ubar, vbar\n",
    "\n",
    "    # Time step: Skip first step to get initial plot\n",
    "    if it > 0\n",
    "        u = step_rk4(dns, u, dt; μ)\n",
    "        vbar = step_rk4(dns, vbar, dt; μ)\n",
    "        ubar = Φ * u\n",
    "    end\n",
    "\n",
    "    # Plot\n",
    "    if it % 50 == 0\n",
    "        sol = plot(;\n",
    "            ylims = (-1.0, 1.0),\n",
    "            legend = :topright,\n",
    "            title = @sprintf(\"Solution, t = %.3f\", it * dt)\n",
    "        )\n",
    "        plot!(sol, x_dns, u; label = \"u\")\n",
    "        plot!(sol, x_les, ubar; label = \"ū\")\n",
    "        plot!(sol, x_les, vbar; label = \"v̄\")\n",
    "        c = plot(\n",
    "            x_les,\n",
    "            Φ * dns(u; μ) - dns(ubar; μ);\n",
    "            label = \"c(u, ū)\",\n",
    "            legend = :topright,\n",
    "            xlabel = \"x\",\n",
    "            ylims = (-10.0, 10.0),\n",
    "        )\n",
    "        fig = plot(sol, c; layout = (2, 1))\n",
    "        frame(anim, fig)\n",
    "    end\n",
    "end\n",
    "gif(anim)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We observe the following:\n",
    "\n",
    "- The DNS solution $u$ contains shocks, but these are not visible in the filtered\n",
    "  DNS solution $\\bar{u}$.\n",
    "- The LES solution $\\bar{v}$ does try to resolve the shocks, but does a very\n",
    "  bad job. The discretization $f_\\text{central}$ is also creating oscillations.\n",
    "- We see that there is a clear mismatch between $\\bar{u}$ (target) and $\\bar{v}$\n",
    "  (LES prediction). This is because we did not take into account the commutator\n",
    "  error (bottom curve).\n",
    "- The commutator error is large where $u$ has a shock, and small everywhere\n",
    "  else. The \"subgrid content\" for the 1D Burgers equations is the stress\n",
    "  caused by the subgrid shocks.\n",
    "\n",
    "_The job of the closure model is to predict the bottom curve using the smooth\n",
    "orange top curve only._\n",
    "\n",
    "We now create the training, validation, and testing datasets.\n",
    "We use a slightly different time step for testing to detect time step overfitting."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "μ = 5.0e-4\n",
    "data_train = create_data(10, Φ; μ, nt = 2000, dt = 1.0e-4);\n",
    "data_valid = create_data(2, Φ; μ, nt = 500, dt = 1.3e-4);\n",
    "data_test = create_data(3, Φ; μ, nt = 3000, dt = 1.1e-4);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The information about our problem is now fully contained in the data sets. We\n",
    "can now choose the closure model to solve the problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss function\n",
    "\n",
    "To choose $\\theta$, we will minimize a loss function using a gradient\n",
    "descent based optimization method (\"train\" the neural network).\n",
    "Since the model is used to predict the commutator error, the obvious choice\n",
    "of loss function is the prior loss function\n",
    "\n",
    "$$\n",
    "L^\\text{prior}(\\theta) = \\| m(\\bar{u}, \\theta) - c(u, \\bar{u}) \\|^2.\n",
    "$$\n",
    "\n",
    "This loss function has a simple computational chain, that is mostly comprised\n",
    "of evaluating the neural network $m$ itself. Computing the gradient with\n",
    "respect to $\\theta$ is thus simple. The gradient is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} L^\\text{prior}}{\\mathrm{d} \\theta}(\\theta) = 2 (m(\\bar{u},\n",
    "\\theta) - c(u, \\bar{u}))^\\mathsf{T}\n",
    "\\frac{\\partial m}{\\partial \\theta}(\\bar{u}, \\theta),\n",
    "$$\n",
    "\n",
    "but we don't need to specify any of that, Zygote figures it out just fine on\n",
    "its own.\n",
    "\n",
    "We call this loss function \"prior\" since it only measures the error of the\n",
    "prediction itself, and not the effect this error has on the LES\n",
    "solution $\\bar{v}_{\\theta}$. Since instability in $\\bar{v}_{\\theta}$ is not\n",
    "directly detected in this loss function, we add a regularization term to\n",
    "penalize extremely large weights."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mean_squared_error(m, u, c, θ; λ) =\n",
    "    sum(abs2, m(u, θ) - c) / sum(abs2, c) + λ * sum(abs2, θ) / length(θ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will only use a random subset `nuse` of all `nsample * nt`\n",
    "solution snapshots at each loss evaluation. This random sampling creates a\n",
    "random variable in the loss function, which becomes stochastic. Minimizing it\n",
    "using gradient descent is thus called _stochastic gradient descent_.\n",
    "The `Zygote.@ignore` macro just tells the AD engine Zygote not to complain\n",
    "about the random index selection."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function create_randloss_commutator(m, data; nuse = 20, λ = 1.0e-8)\n",
    "    (; u, c) = data\n",
    "    u = reshape(u, size(u, 1), :)\n",
    "    c = reshape(c, size(c, 1), :)\n",
    "    nsample = size(u, 2)\n",
    "    @assert nsample ≥ nuse\n",
    "    function randloss(θ)\n",
    "        i = Zygote.@ignore sort(shuffle(1:nsample)[1:nuse])\n",
    "        uuse = Zygote.@ignore u[:, i]\n",
    "        cuse = Zygote.@ignore c[:, i]\n",
    "        mean_squared_error(m, uuse, cuse, θ; λ)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ideally, we want the LES simulation to produce the filtered DNS velocity\n",
    "$\\bar{u}$. The prior loss does not guarantee or enforce this.\n",
    "We can alternatively minimize the posterior loss function\n",
    "\n",
    "$$\n",
    "L^\\text{post}(\\theta) = \\| \\bar{v}_\\theta - \\bar{u} \\|^2,\n",
    "$$\n",
    "\n",
    "where $\\bar{v}_\\theta$ is the solution to the LES equation for the given\n",
    "parameters. This loss function contains more information about the effect of\n",
    "$\\theta$ than $L^\\text{prior}$. However, it has a significantly longer\n",
    "computational chain, as it includes time stepping in addition to the neural\n",
    "network evaluation itself. Computing the gradient with respect to $\\theta$ is\n",
    "more costly, and also requires an AD-friendly time stepping scheme (which we\n",
    "have already taken care of above). Note that it is also possible to compute\n",
    "the gradient of the time-continuous ODE instead of the time-discretized one\n",
    "as we do here. It involves solving an adjoint ODE backwards in time, which in\n",
    "turn has to be discretized. Our approach here is therefore called\n",
    "\"discretize-then-optimize\", while the adjoint ODE method is called\n",
    "\"optimize-then-discretize\". The [SciML](https://github.com/sciml) time\n",
    "steppers include both methods, as well as useful strategies for evaluating\n",
    "them efficiently.\n",
    "\n",
    "For the posterior loss function, we provide the right hand side function\n",
    "`model` (including closure), reference trajectories `u`, and model\n",
    "parameters. We compute the error between the predicted and reference trajectories\n",
    "at each time point."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function trajectory_loss(model, u; dt, params...)\n",
    "    nt = size(u, 3) - 1\n",
    "    loss = 0.0\n",
    "    v = u[:, :, 1]\n",
    "    for i = 1:nt\n",
    "        v = step_rk4(model, v, dt; params...)\n",
    "        ui = u[:, :, i+1]\n",
    "        loss += sum(abs2, v - ui) / sum(abs2, ui)\n",
    "    end\n",
    "    loss / nt\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also make a non-squared variant for error analysis."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function trajectory_error(model, u; dt, params...)\n",
    "    nt = size(u, 3) - 1\n",
    "    loss = 0.0\n",
    "    v = u[:, :, 1]\n",
    "    for i = 1:nt\n",
    "        v = step_rk4(model, v, dt; params...)\n",
    "        ui = u[:, :, i+1]\n",
    "        loss += norm(v - ui) / norm(ui)\n",
    "    end\n",
    "    loss / nt\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To limit the length of the computational chain, we only unroll `n_unroll`\n",
    "time steps at each loss evaluation. The time step from which to unroll is\n",
    "chosen at random at each evaluation, as are the initial conditions (`nuse`).\n",
    "\n",
    "The non-trainable parameters (e.g. $\\mu$) are passed in `params`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function create_randloss_trajectory(model, data; nuse = 1, n_unroll = 10, params...)\n",
    "    (; u, dt) = data\n",
    "    nsample = size(u, 2)\n",
    "    nt = size(u, 3)\n",
    "    @assert nt ≥ n_unroll\n",
    "    @assert nsample ≥ nuse\n",
    "    function randloss(θ)\n",
    "        isample = Zygote.@ignore sort(shuffle(1:nsample)[1:nuse])\n",
    "        istart = Zygote.@ignore rand(1:nt-n_unroll)\n",
    "        it = Zygote.@ignore istart:istart+n_unroll\n",
    "        uuse = Zygote.@ignore u[:, isample, it]\n",
    "        trajectory_loss(model, uuse; dt, params..., θ)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training settings\n",
    "\n",
    "During training, we will monitor the error on the validation dataset with a\n",
    "callback. We will plot the history of the prior and posterior errors."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Initial empty history\n",
    "initial_callbackstate() = (; ihist = Int[], ehist_prior = zeros(0), ehist_post = zeros(0))\n",
    "\n",
    "# Plot convergence\n",
    "function plot_convergence(state, data)\n",
    "    e_post_ref = trajectory_error(dns, data.u; data.dt, data.μ)\n",
    "    fig = plot(; yscale = :log10, xlabel = \"Iterations\", title = \"Relative error\")\n",
    "    hline!(fig, [1.0]; color = 1, linestyle = :dash, label = \"Prior: No model\")\n",
    "    plot!(fig, state.ihist, state.ehist_prior; color = 1, label = \"Prior: Model\")\n",
    "    hline!(fig, [e_post_ref]; color = 2, linestyle = :dash, label = \"Posterior: No model\")\n",
    "    plot!(fig, state.ihist, state.ehist_post; color = 2, label = \"Posterior: Model\")\n",
    "    fig\n",
    "end\n",
    "\n",
    "# Create callback for given model and dataset\n",
    "function create_callback(m, data; doplot = false)\n",
    "    (; u, c, dt, μ) = data\n",
    "    uu, cc = reshape(u, size(u, 1), :), reshape(c, size(c, 1), :)\n",
    "    function callback(i, θ, state)\n",
    "        (; ihist, ehist_prior, ehist_post) = state\n",
    "        eprior = norm(m(uu, θ) - cc) / norm(cc)\n",
    "        epost = trajectory_error(les, u; dt, μ, m, θ)\n",
    "        state = (;\n",
    "            ihist = vcat(ihist, i),\n",
    "            ehist_prior = vcat(ehist_prior, eprior),\n",
    "            ehist_post = vcat(ehist_post, epost),\n",
    "        )\n",
    "        doplot && display(plot_convergence(state, data))\n",
    "        @printf \"Iteration %d,\\t\\tprior error: %.4g,\\t\\tposterior error: %.4g\\n\" i eprior epost\n",
    "        state\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For training, we have to initialize an optimizer and a callbackstate (lots of\n",
    "state initilization in this session)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "initial_trainstate(optimiser, θ) = (;\n",
    "    opt = Optimisers.setup(optimiser, θ),\n",
    "    θ,\n",
    "    callbackstate = initial_callbackstate(),\n",
    "    istart = 0,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "    train(; loss, opt, θ, istart, niter, ncallback, callback, callbackstate)\n",
    "\n",
    "Perform `niter` iterations of gradient descent on `loss(θ)` for given `optimiser`.\n",
    "Every `ncallback` iteration, call `callback(i, θ, callbackstate)` and update\n",
    "callback state, where `i` starts at `istart`.\n",
    "\n",
    "Return a named tuple `(; opt, θ, callbcackstate)` containing the updated\n",
    "optimizer state, parameters and callback state.\n",
    "\"\"\"\n",
    "function train(; loss, opt, θ, istart, niter, ncallback, callback, callbackstate)\n",
    "    for i = 1:niter\n",
    "        ∇ = first(gradient(loss, θ))\n",
    "        opt, θ = Optimisers.update(opt, θ, ∇)\n",
    "        i % ncallback == 0 && (callbackstate = callback(istart + i, θ, callbackstate))\n",
    "    end\n",
    "    istart += niter\n",
    "    (; opt, θ, callbackstate, istart)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model architecture\n",
    "\n",
    "We are free to choose the model architecture $m$. Here, we will consider two\n",
    "neural network architectures. The following wrapper returns the model and\n",
    "initial parameters for a Lux `Chain`. Note: If the chain includes\n",
    "state-dependent layers such as `Dropout` (which modify their RNGs at each\n",
    "evaluation), this wrapper should not be used."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function create_model(chain, rng)\n",
    "    # Create parameter vector and empty state\n",
    "    θ, state = Lux.setup(rng, chain)\n",
    "\n",
    "    # Convert nested named tuples of arrays to a ComponentArray,\n",
    "    # which behaves like a long vector\n",
    "    θ = ComponentArray(θ)\n",
    "\n",
    "    # Convenience wrapper for empty state in input and output\n",
    "    m(v, θ) = first(chain(v, θ, state))\n",
    "\n",
    "    # Return model and initial parameters\n",
    "    m, θ\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convolutional neural network architecture (CNN)\n",
    "\n",
    "A CNN is an interesting closure model for the following reasons:\n",
    "\n",
    "- The parameters are sparse, since the kernels are reused for each output\n",
    "- A convolutional layer can be seen as a discretized differential operator\n",
    "- Translation invariance, a desired physical property of the commutator\n",
    "  error we try to predict, is baked in.\n",
    "\n",
    "However, it only uses local spatial information, whereas an ideal closure\n",
    "model could maybe recover some of the missing information in far-away values.\n",
    "\n",
    "Note that we start by adding input channels, stored in a tuple of functions.\n",
    "The Burgers RHS has a square term, so maybe the closure model can make use of\n",
    "the same \"structure\". See Melchers [^2]."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "    create_cnn(;\n",
    "        radii,\n",
    "        channels,\n",
    "        activations,\n",
    "        use_bias,\n",
    "        rng,\n",
    "        input_channels = (u -> u,),\n",
    "    )\n",
    "\n",
    "Create CNN.\n",
    "\n",
    "Keyword arguments:\n",
    "\n",
    "- `radii`: Vector of kernel radii\n",
    "- `channels`: Vector layer output channel numbers\n",
    "- `activations`: Vector of activation functions\n",
    "- `use_bias`: Vectors of indicators for using bias\n",
    "- `rng`: Random number generator\n",
    "- `input_channels`: Tuple of input channel contstructors\n",
    "\n",
    "Return `(cnn, θ)`, where `cnn(v, θ)` acts like a force on `v`.\n",
    "\"\"\"\n",
    "function create_cnn(;\n",
    "    radii,\n",
    "    channels,\n",
    "    activations,\n",
    "    use_bias,\n",
    "    rng,\n",
    "    input_channels = (u -> u,),\n",
    ")\n",
    "    @assert channels[end] == 1 \"A unique output channel is required\"\n",
    "\n",
    "    # Add number of input channels\n",
    "    channels = [length(input_channels); channels]\n",
    "\n",
    "    # Padding length\n",
    "    padding = sum(radii)\n",
    "\n",
    "    # Create CNN\n",
    "    create_model(\n",
    "        Chain(\n",
    "            # Create singleton channel\n",
    "            u -> reshape(u, size(u, 1), 1, size(u, 2)),\n",
    "\n",
    "            # Create input channels\n",
    "            u -> hcat(map(i -> i(u), input_channels)...),\n",
    "\n",
    "            # Add padding so that output has same shape as commutator error\n",
    "            u -> pad_circular(u, padding; dims = 1),\n",
    "\n",
    "            # Some convolutional layers\n",
    "            (\n",
    "                Conv(\n",
    "                    (2 * radii[i] + 1,),\n",
    "                    channels[i] => channels[i+1],\n",
    "                    activations[i];\n",
    "                    use_bias = use_bias[i],\n",
    "                    init_weight = glorot_uniform_64,\n",
    "                ) for i ∈ eachindex(radii)\n",
    "            )...,\n",
    "\n",
    "            # Remove singleton output channel\n",
    "            u -> reshape(u, size(u, 1), size(u, 3)),\n",
    "        ),\n",
    "        rng,\n",
    "    )\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fourier neural operator architecture (FNO)\n",
    "\n",
    "A Fourier neural operator [^3] is a network composed of _Fourier Layers_ (FL).\n",
    "A Fourier layer $u \\mapsto w$ transforms the continuous function $u$\n",
    "into the contiunous function $w$.\n",
    "It is defined by the following expression in physical space:\n",
    "\n",
    "$$\n",
    "w(x) = \\sigma \\left( z(x) + W u(x) \\right), \\quad \\forall x \\in \\Omega,\n",
    "$$\n",
    "\n",
    "where $z$ is defined by its Fourier series coefficients $\\hat{z}(k) = R(k)\n",
    "\\hat{u}(k)$ for all wavenumbers $k \\in \\mathbb{Z}$ and some weight matrix\n",
    "collection $R(k) \\in \\mathbb{C}^{n_\\text{out} \\times n_\\text{in}}$. The\n",
    "important part is the following choice: $R(k) = 0$ for $| k | >\n",
    "k_\\text{max}$ for some $k_\\text{max}$. This truncation makes the FNO\n",
    "applicable to any spatial $N$-discretization of $u$ and $w$ as long as $N > 2\n",
    "k_\\text{max}$. The same weight matrices may be reused for different\n",
    "discretizations.\n",
    "\n",
    "Note that a standard convolutional layer (CL) can also be written in spectral\n",
    "space, where the spatial convolution operation becomes an wavenumber\n",
    "element-wise product. The effective difference between the layers of a FNO\n",
    "and CNN becomes the following:\n",
    "\n",
    "- A FL does not include the bias of a CL\n",
    "- The spatial part of a FL corresponds to the central weight of the CL\n",
    "  kernel\n",
    "- A CL is chosen to be sparse in physical space (local kernel with radius $r\n",
    "  \\ll N / 2$), and would therefore be dense in spectral space ($k_\\text{max} =\n",
    "  N / 2$)\n",
    "- The spectral part of a FL is chosen to be sparse in spectral space\n",
    "  ($k_\\text{max} \\ll N / 2$), and would therefore dense in physical space (it\n",
    "  can be written as a convolution stencil with radius $r = N / 2$)\n",
    "\n",
    "Lux lets us define\n",
    "our own layer types. All functions should be \"pure\" (functional programming),\n",
    "meaning that the same inputs should produce the same outputs. In particular,\n",
    "this also applies to random number generation and state modification. The\n",
    "weights are stored in a vector outside the layer, while the layer itself\n",
    "contains information for weight initialization.\n",
    "\n",
    "If you are not familiar with Julia, feel free to go quickly through\n",
    "the following code cells. Just note that all variables have a type (e.g.\n",
    "`kmax::Int` means that `kmax` is an integer), but most of the time we don't have\n",
    "to declare types explicitly. Structures (`struct`s) can be\n",
    "parametrized and specialized for the types we give them in the constructor\n",
    "(e.g. `σ::A` means that a specialized version of the struct is compiled for\n",
    "each activation function we give it, creating an optimized FourierLayer for\n",
    "`σ = relu` where `A = typeof(relu)`, and a different version optimized for `σ\n",
    "= tanh` where `A = typeof(tanh)` etc.). Here our layer will have the type\n",
    "`FourierLayer`, with a default and custom constructor (two constructor\n",
    "methods, the latter making use of the default)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct FourierLayer{A,F} <: Lux.AbstractExplicitLayer\n",
    "    kmax::Int\n",
    "    cin::Int\n",
    "    cout::Int\n",
    "    σ::A\n",
    "    init_weight::F\n",
    "end\n",
    "\n",
    "FourierLayer(kmax, ch::Pair{Int,Int}; σ = identity, init_weight = glorot_uniform_64) =\n",
    "    FourierLayer(kmax, first(ch), last(ch), σ, init_weight)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need to specify how to initialize the parameters and states. The\n",
    "Fourier layer does not have any hidden states that are modified. The below\n",
    "code adds methods to some existing Lux functions. These new methods are only\n",
    "used when the functions encounter `FourierLayer` inputs. For example, in our\n",
    "current environment, we have this many methods for the function\n",
    "`Lux.initialparameters` (including `Dense`, `Conv`, etc.):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "length(methods(Lux.initialparameters))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, when we add our own method, there should be one more in the method\n",
    "table."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Lux.initialparameters(rng::AbstractRNG, (; kmax, cin, cout, init_weight)::FourierLayer) = (;\n",
    "    spatial_weight = init_weight(rng, cout, cin),\n",
    "    spectral_weights = init_weight(rng, kmax + 1, cout, cin, 2),\n",
    ")\n",
    "Lux.initialstates(::AbstractRNG, ::FourierLayer) = (;)\n",
    "Lux.parameterlength((; kmax, cin, cout)::FourierLayer) =\n",
    "    cout * cin + (kmax + 1) * 2 * cout * cin\n",
    "Lux.statelength(::FourierLayer) = 0\n",
    "\n",
    "# Pretty printing\n",
    "function Base.show(io::IO, (; kmax, cin, cout, σ)::FourierLayer)\n",
    "    print(io, \"FourierLayer(\", kmax)\n",
    "    print(io, \", \", cin, \" => \", cout)\n",
    "    print(io, \"; σ = \", σ)\n",
    "    print(io, \")\")\n",
    "end\n",
    "\n",
    "# One more method now\n",
    "length(methods(Lux.initialparameters))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is one of the advantages of Julia: As users we can extend functions from\n",
    "other authors without modifying their package or being forced to \"inherit\"\n",
    "their data structures (classes). This has created an interesting package\n",
    "ecosystem. For example, [ODE\n",
    "solvers](https://github.com/SciML/OrdinaryDiffEq.jl) can be used with exotic\n",
    "number types such as `BigFloat`, dual numbers, or quaternions. [Iterative\n",
    "solvers](https://github.com/JuliaLinearAlgebra/IterativeSolvers.jl)\n",
    "work out of the box with different array types, including various [GPU\n",
    "arrays](https://github.com/JuliaGPU/GPUArrays.jl), without actually\n",
    "containing any GPU array specific code.\n",
    "\n",
    "We now define how to pass inputs through a Fourier layer. In tensor notation,\n",
    "multiple samples can be processed at the same time. We therefore assume the\n",
    "following:\n",
    "\n",
    "- Input size: `(nx, cin, nsample)`\n",
    "- Output size: `(nx, cout, nsample)`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# This makes FourierLayers callable\n",
    "function ((; kmax, cout, cin, σ)::FourierLayer)(x, params, state)\n",
    "    nx = size(x, 1)\n",
    "\n",
    "    # Destructure params\n",
    "    # The real and imaginary parts of R are stored in two separate channels\n",
    "    W = params.spatial_weight\n",
    "    W = reshape(W, 1, cout, cin)\n",
    "    R = params.spectral_weights\n",
    "    R = selectdim(R, 4, 1) .+ im .* selectdim(R, 4, 2)\n",
    "\n",
    "    # Spatial part (applied point-wise)\n",
    "    y = reshape(x, nx, 1, cin, :)\n",
    "    y = sum(W .* y; dims = 3)\n",
    "    y = reshape(y, nx, cout, :)\n",
    "\n",
    "    # Spectral part (applied mode-wise)\n",
    "    #\n",
    "    # Steps:\n",
    "    #\n",
    "    # - go to complex-valued spectral space\n",
    "    # - chop off high wavenumbers\n",
    "    # - multiply with weights mode-wise\n",
    "    # - pad with zeros to restore original shape\n",
    "    # - go back to real valued spatial representation\n",
    "    ikeep = 1:kmax+1\n",
    "    nkeep = kmax + 1\n",
    "    z = rfft(x, 1)\n",
    "    z = z[ikeep, :, :]\n",
    "    z = reshape(z, nkeep, 1, cin, :)\n",
    "    z = sum(R .* z; dims = 3)\n",
    "    z = reshape(z, nkeep, cout, :)\n",
    "    z = vcat(z, zeros(nx ÷ 2 + 1 - kmax - 1, size(z, 2), size(z, 3)))\n",
    "    z = irfft(z, nx, 1)\n",
    "\n",
    "    # Outer layer: Activation over combined spatial and spectral parts\n",
    "    # Note: Even though high wavenumbers are chopped off in `z` and may\n",
    "    # possibly not be present in the input at all, `σ` creates new high\n",
    "    # wavenumbers. High wavenumber functions may thus be represented using a\n",
    "    # sequence of Fourier layers. In this case, the `y`s are the only place\n",
    "    # where information contained in high input wavenumbers survive in a\n",
    "    # Fourier layer.\n",
    "    w = σ.(z .+ y)\n",
    "\n",
    "    # Fourier layer does not modify state\n",
    "    w, state\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will chain some Fourier layers, with a final dense layer. As for the CNN,\n",
    "we allow for a tuple of predetermined input channels."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "    create_fno(; channels, kmax, activations, rng, input_channels = (u -> u,))\n",
    "\n",
    "Create FNO.\n",
    "\n",
    "Keyword arguments:\n",
    "\n",
    "- `channels`: Vector of output channel numbers\n",
    "- `kmax`: Vector of cut-off wavenumbers\n",
    "- `activations`: Vector of activation functions\n",
    "- `rng`: Random number generator\n",
    "- `input_channels`: Tuple of input channel constructors\n",
    "\n",
    "Return `(fno, θ)`, where `fno(v, θ)` acts like a force on `v`.\n",
    "\"\"\"\n",
    "function create_fno(; channels, kmax, activations, rng, input_channels = (u -> u,))\n",
    "    # Add number of input channels\n",
    "    channels = [length(input_channels); channels]\n",
    "\n",
    "    # Model\n",
    "    create_model(\n",
    "        Chain(\n",
    "            # Create singleton channel\n",
    "            u -> reshape(u, size(u, 1), 1, size(u, 2)),\n",
    "\n",
    "            # Create input channels\n",
    "            u -> hcat(map(i -> i(u), input_channels)...),\n",
    "\n",
    "            # Some Fourier layers\n",
    "            (\n",
    "                FourierLayer(kmax[i], channels[i] => channels[i+1]; σ = activations[i]) for\n",
    "                i ∈ eachindex(kmax)\n",
    "            )...,\n",
    "\n",
    "            # Put channels in first dimension\n",
    "            u -> permutedims(u, (2, 1, 3)),\n",
    "\n",
    "            # Compress with a final dense layer\n",
    "            Dense(channels[end] => 2 * channels[end], gelu),\n",
    "            Dense(2 * channels[end] => 1; use_bias = false),\n",
    "\n",
    "            # Put channels back after spatial dimension\n",
    "            u -> permutedims(u, (2, 1, 3)),\n",
    "\n",
    "            # Remove singleton channel\n",
    "            u -> reshape(u, size(u, 1), size(u, 3)),\n",
    "        ),\n",
    "        rng,\n",
    "    )\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting to business: Training and comparing closure models\n",
    "\n",
    "We now create a closure model. Note that the last activation is `identity`, as we\n",
    "don't want to restrict the output values. We can inspect the structure in the\n",
    "wrapped Lux `Chain`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m_cnn, θ_cnn = create_cnn(;\n",
    "    radii = [2, 2, 2, 2],\n",
    "    channels = [8, 8, 8, 1],\n",
    "    activations = [leakyrelu, leakyrelu, leakyrelu, identity],\n",
    "    use_bias = [true, true, true, false],\n",
    "    input_channels = (u -> u, u -> u .^ 2),\n",
    "    rng,\n",
    ")\n",
    "m_cnn.chain"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m_fno, θ_fno = create_fno(;\n",
    "    channels = [5, 5, 5, 5],\n",
    "    kmax = [16, 16, 16, 8],\n",
    "    activations = [gelu, gelu, gelu, identity],\n",
    "    input_channels = (u -> u, u -> u .^ 2),\n",
    "    rng,\n",
    ")\n",
    "m_fno.chain"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m, θ, label = m_cnn, θ_cnn, \"CNN\";\n",
    "# m, θ, label = m_fno, θ_fno, \"FNO\";"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Choose loss"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss_prior = create_randloss_commutator(m, data_train; nuse = 50);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss_post =\n",
    "    create_randloss_trajectory(les, data_train; nuse = 3, n_unroll = 10, data_train.μ, m);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss = loss_prior;\n",
    "# loss = loss_post;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize training state. Note that we have to provide an optimizer, here\n",
    "`Adam(η)` where `η` is the learning rate [^4]. This optimizer exploits the\n",
    "random nature of our loss function."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "trainstate = initial_trainstate(Adam(1.0e-3), θ);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model warm-up: trigger compilation and get indication of complexity"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss(θ)\n",
    "gradient(loss, θ);\n",
    "@time loss(θ);\n",
    "@time gradient(loss, θ);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model. The cell below can be repeated to continue training where the\n",
    "previous training session left off.\n",
    "If you run this in a notebook, `doplot = true` will create a lot of plots\n",
    "below the cell."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "trainstate = train(;\n",
    "    trainstate...,\n",
    "    loss,\n",
    "    niter = 1000,\n",
    "    ncallback = 20,\n",
    "    callback = create_callback(m, data_valid; doplot = false),\n",
    ")\n",
    "plot_convergence(trainstate.callbackstate, data_valid)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Final model weights"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "(; θ) = trainstate;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model performance\n",
    "\n",
    "We will now make a comparison between our closure model, the baseline \"no closure\" model,\n",
    "and the reference testing data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "println(\"Relative posterior errors:\")\n",
    "e0 = trajectory_error(dns, data_test.u; data_test.dt, data_test.μ)\n",
    "e = trajectory_error(les, data_test.u; data_test.dt, data_test.μ, m, θ)\n",
    "println(\"m=0:\\t$e0\")\n",
    "println(\"$label:\\t$e\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also plot the LES solutions with and without closure model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "(; u, dt, μ) = data_test\n",
    "u = u[:, 1, :]\n",
    "nt = size(u, 2) - 1\n",
    "v0 = u[:, 1]\n",
    "v = u[:, 1]\n",
    "anim = Animation()\n",
    "for it = 0:nt\n",
    "    # Only needed if we are not running interactively\n",
    "    # (since we are not inside a function) e.g. on GitHub\n",
    "    # Note: These variables are still local to the cell\n",
    "    global v0, v\n",
    "\n",
    "    # Time step: Skip first step to get initial plot\n",
    "    if it > 0\n",
    "        v0 = step_rk4(dns, v0, dt; μ)\n",
    "        v = step_rk4(les, v, dt; μ, m, θ)\n",
    "    end\n",
    "\n",
    "    # Plot\n",
    "    if it % 50 == 0\n",
    "        fig = plot(;\n",
    "            ylims = extrema(u[:, 1]),\n",
    "            xlabel = \"x\",\n",
    "            title = @sprintf(\"Solution, t = %.3f\", it * dt),\n",
    "            legend = :topright,\n",
    "        )\n",
    "        plot!(fig, x_les, u[:, it+1]; label = \"Ref\")\n",
    "        plot!(fig, x_les, v0; label = \"m=0\")\n",
    "        plot!(fig, x_les, v; label)\n",
    "        frame(anim, fig)\n",
    "    end\n",
    "end\n",
    "gif(anim)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling exercises\n",
    "\n",
    "To get confident with modeling ODE right hand sides using machine learning,\n",
    "the following exercises can be useful.\n",
    "\n",
    "### 1. Trajectory fitting (posterior loss function)\n",
    "\n",
    "1. Fit a closure model using the posterior loss function.\n",
    "1. Investigate the effect of the parameter `n_unroll`. Try for example\n",
    "   `@time randloss(θ)` for `n_unroll = 10` and `n_unroll = 20`\n",
    "   (execute `randloss` once first to trigger compilation).\n",
    "1. Discuss the statement \"$L^\\text{prior}$ and $L^\\text{post}$ are almost the\n",
    "   same when `n_unroll = 1`\" with your neighbour. Are they exactly the same if\n",
    "   we use forward Euler ($u^{n + 1} = u^n + \\Delta t f(u^n)$) instead of RK4?\n",
    "\n",
    "### 2. Naive neural closure model\n",
    "\n",
    "Recreate the CNN or FNO model without the additional square input channel\n",
    "(prior physical knowledge).\n",
    "The input should only have one singleton channel (pass the\n",
    "keyword argument `input_channels = (u -> u,)` to the constructor).\n",
    "Do you expect this version to perform better or worse than with a square\n",
    "channel?\n",
    "\n",
    "### 3. Neural ODE (brute force right hand side)\n",
    "\n",
    "1. Observe that, if we really wanted to, we could skip the term $f(\\bar{v})$\n",
    "   entirely, hoping that $m$ will be able to model it directly (in addition\n",
    "   to the commutator error). The resulting model is\n",
    "\n",
    "   $$\n",
    "   \\frac{\\mathrm{d} \\bar{v}}{\\mathrm{d} t} = m(\\bar{v}, \\theta).\n",
    "   $$\n",
    "\n",
    "   This is known as a _Neural ODE_ (see Chen [^5]).\n",
    "1. Define a model that predicts the _entire_ right hand side.\n",
    "   This can be done by using the following little \"hack\":\n",
    "\n",
    "   - Create your model `m_entire_rhs, θ_entire_rhs = create_cnn(...)`\n",
    "   - Define the effective closure model\n",
    "     `m_effective(u, θ) = m_entire_rhs(u, θ) - dns(u; μ)`.\n",
    "\n",
    "1. Train the CNN or FNO for `m_effective`.\n",
    "   Is the model able to represent the solution correctly?\n",
    "\n",
    "### 4. Learning the discretization\n",
    "\n",
    "1. Make a new instance of the CNN closure, called `cnn_linear` with parameters\n",
    "   `θ_cnn_linear`, which only has one convolutional layer.\n",
    "   This model should still add the square input channel.\n",
    "1. Observe that the simple Burgers DNS RHS $f_\\text{central}$ can actually\n",
    "   be expressed in its entirety using this model, i.e.\n",
    "\n",
    "   $$\n",
    "   f_\\text{central}(u) = \\mathop{\\text{CNN}}(u, \\theta).\n",
    "   $$\n",
    "\n",
    "   - What is the kernel radius?\n",
    "   - Should there still be a nonlinear activation function?\n",
    "   - What is the exact expression for the model weights and bias?\n",
    "\n",
    "1. \"Improve\" the discretization $f_\\text{central}$:\n",
    "\n",
    "   - Choose a kernel radius for `cnn_linear` that is larger than the\n",
    "     one of `f_central`.\n",
    "   - Train the model.\n",
    "   - What does the resulting kernel stencil (`Array(θ_cnn_linear)`) look like?\n",
    "     Does it resemble the one of `f_central`?\n",
    "\n",
    "1. Observe that if we really wanted to, we could do the same for the more\n",
    "   complicated Burgers right hand side $f_\\text{shock}$. Some extra\n",
    "   difficulties in this version of $f$:\n",
    "\n",
    "   - It contains an absolute value (in the numerical viscosity)\n",
    "   - It contains cross terms $u_n u_{n + 1}$ (instead of element-wise squares\n",
    "     $u_n^2$)\n",
    "\n",
    "   If we use a CNN with two layers (instead of one) and with\n",
    "   $\\sigma(x) = \\mathop{\\text{relu}}(x) = \\max(0, x)$ as an activation\n",
    "   function between the two layers, then the absolute value can be written\n",
    "   as the sum of two $\\mathop{\\text{relu}}$ functions with\n",
    "   $|x| = \\mathop{\\text{relu}}(x) + \\mathop{\\text{relu}}(-x)$.\n",
    "   Expressions on the same level without absolute value can also\n",
    "   be written as a sum of two $\\mathop{\\text{relu}}$ functions with\n",
    "   $x = \\mathop{\\text{relu}}(x) - \\mathop{\\text{relu}}(-x)$.\n",
    "   For the cross term, we just add it in the input channel layer.\n",
    "\n",
    "So, a simple discretization $f_\\text{central}$ can be written as a one layer\n",
    "CNN. A better discretization $f_\\text{shock}$ can be written as a\n",
    "two layer CNN... What can we do with an $n$ layer CNN?\n",
    "\n",
    "Final question: What is a closure model? Are we just learning a better\n",
    "coarse discretization? Discuss with your neighbour.\n",
    "\n",
    "## References\n",
    "\n",
    "[^1]: Antony Jameson,\n",
    "      _Energy Estimates for Nonlinear Conservation Laws with Applications to\n",
    "      Solutions of the Burgers Equation and One-Dimensional Vicous Flow in a\n",
    "      Shock Tube by Central Difference Schemes_,\n",
    "      18th AIAA Computational Fluid Dynamics Conference,\n",
    "      2007,\n",
    "      <https://doi.org/10.2514/6.2007-4620>\n",
    "\n",
    "[^2]: Hugo Melchers, Daan Crommelin, Barry Koren, Vlado Menkovski, Benjamin\n",
    "      Sanderse,\n",
    "      _Comparison of neural closure models for discretised PDEs_,\n",
    "      Computers & Mathematics with Applications,\n",
    "      Volume 143,\n",
    "      2023,\n",
    "      Pages 94-107,\n",
    "      ISSN 0898-1221,\n",
    "      <https://doi.org/10.1016/j.camwa.2023.04.030>.\n",
    "\n",
    "[^3]: Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A.\n",
    "      Stuart, and A. Anandkumar.\n",
    "      _Fourier neural operator for parametric partial differential\n",
    "      equations._\n",
    "      arXiv:[2010.08895](https://arxiv.org/abs/2010.08895),\n",
    "      2021.\n",
    "\n",
    "[^4]: D. P. Kingma and J. Ba.\n",
    "      _Adam: A method for stochastic optimization_.\n",
    "      arxiv:[1412.6980](https://arxiv.org/abs/1412.6980),\n",
    "      2014.\n",
    "\n",
    "[^5]: R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud.\n",
    "      _Neural Ordinary Differential Equations_.\n",
    "      arXiv:[1806.07366](https://arxiv.org/abs/1806.07366),\n",
    "      2018."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "kernelspec": {
   "name": "julia-1.9",
   "display_name": "Julia 1.9.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
